{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SelAw432/DeepLearning/blob/main/Practical2/ThingsWillGoWrongAgain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Practical 2 - Part 3 - Things will go wrong again!\n",
        "---\n",
        "\n",
        "## Author : Amir Atapour-Abarghouei, amir.atapour-abarghouei@durham.ac.uk\n",
        "\n",
        "This notebook will provide you with another exercise in identifying issues when building and training a simple nerual network.\n",
        "\n",
        "Copyright (c) 2022 Amir Atapour-Abarghouei, UK.\n",
        "\n",
        "License : LGPL - http://www.gnu.org/licenses/lgpl.html"
      ],
      "metadata": {
        "id": "8KdYlhYxFxqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like [our last exercise](https://colab.research.google.com/gist/atapour/11bbb081bcac21a45aa1bf985c644d36#scrollTo=caZAL4nPAV2H), we are going to have the entire setup and training loop in one cell. This time, though, we are going to use the FashionMNIST dataset, but to speed up the process, we will use a large batch size.\n",
        "\n",
        "In fact, the \"client\" that has hired us to complete the project has insisted that we use a batch size of 1950 and the input images have to be resized to 128x128 - *they have their reasons, don't poke at the scenario too much!!! :o)*. We should hopefully be able to change any other parameter we want.\n",
        "\n",
        "The point is trying to use as much of the GPU memory without actually going over the limit. This has been test on Google Colab and NCC Jupyter Hub (on the res partition). On a different GPU, you might run out of memeory right off the bat. That is not meant to happen, so you may have to adjust the batch size and image resolution so you don't run out of memeory before training starts.\n",
        "\n",
        "Make sure you go through and understand every part of the code. Ask questions if there is something you don't understand."
      ],
      "metadata": {
        "id": "3CpSoNObGAnp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VgUSlywExeU"
      },
      "outputs": [],
      "source": [
        "!pip install livelossplot --quiet\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from livelossplot import PlotLosses\n",
        "import os.path\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f'Device is {device}!')\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.FashionMNIST('data', train=True, download=True, transform=torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize(128),\n",
        "        torchvision.transforms.Grayscale(num_output_channels=3),\n",
        "        torchvision.transforms.ToTensor()\n",
        "    ])),\n",
        "shuffle=True, batch_size=1950)\n",
        "\n",
        "print('created the dataloader for training set!')\n",
        "\n",
        "# we will use a ResNet model here - you will learn about these later\n",
        "# but you can see how easy CNNs are to use in PyTorch\n",
        "model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "num_infea = model.fc.in_features\n",
        "model.fc = nn.Linear(num_infea, 10)\n",
        "model = model.to(device)\n",
        "\n",
        "# create the optimiser:\n",
        "optimiser = torch.optim.SGD(model.parameters(), lr=9.9)\n",
        "\n",
        "# since this is a binary problem, we will use binary cross entropy as the loss function\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# to calculate the total loss over entire training:\n",
        "total_loss = 0\n",
        "\n",
        "# initialising epoch variable\n",
        "epoch = 0\n",
        "\n",
        "# to plot losses\n",
        "liveloss = PlotLosses()\n",
        "\n",
        "# to keep the logs for loss plots\n",
        "logs = {}\n",
        "\n",
        "# main training loop:\n",
        "while epoch < 5:\n",
        "\n",
        "    for j, batch in enumerate(train_loader):\n",
        "\n",
        "        print(f'Training step: {j}')\n",
        "\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        total_loss += loss\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "        # calculating the accuracy\n",
        "        _, argmax = torch.max(output, dim=1)\n",
        "        accuracy = argmax.eq(y).float().mean() * 100\n",
        "\n",
        "    logs['Loss'] = loss.item()\n",
        "    logs['Accuracy'] = accuracy.item()\n",
        "    liveloss.update(logs)\n",
        "    liveloss.send()\n",
        "    print(f'Total loss is {total_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should \"hopefully\" see that after a number of steps, you are getting an error. How can you be getting such an error after the model has correctly trained for a few steps? What is this error? Why is it happening?\n",
        "\n",
        "Try to investigate and find the issue."
      ],
      "metadata": {
        "id": "DsTUV4jpDk3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use your favourite debugging techniques and find out what the issue could be.\n",
        "\n",
        "You may also find the following material useful in your quest:\n",
        "\n",
        "https://pytorch.org/docs/stable/notes/faq.html"
      ],
      "metadata": {
        "id": "N7BdKDmfL6M-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right! Hopefully by this point, you will have fixed the error and your model should be training - if not ask for help!"
      ],
      "metadata": {
        "id": "ApHVZSRQMOSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, fixing errors is just half the battle. Even if the error is gone, you should notice that the model is not actually training and performing well.\n",
        "\n",
        "Try to investigate further and find the issue there."
      ],
      "metadata": {
        "id": "qT3NC-HdMZM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following material should help you understand the issue:\n",
        "\n",
        "https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/\n",
        "(the practical examples here use Keras - but don't mind that, PyTorch is better!)"
      ],
      "metadata": {
        "id": "NcS144KmOjU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So go back and fix the issue and make sure that the model is training well!"
      ],
      "metadata": {
        "id": "Ob3y32kFQh1I"
      }
    }
  ]
}